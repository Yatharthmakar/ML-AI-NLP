{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXKfxbER5jQZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_gradient(X_batch, y_batch, theta):\n",
        "#   n_samples = X_batch.shape[0]\n",
        "#   predictions = X_batch.dot(theta)\n",
        "#   error = predictions - y_batch\n",
        "#   gradient = X_batch.T.dot(error) / n_samples\n",
        "#   return gradient"
      ],
      "metadata": {
        "id": "ass9bgs2vV40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=100, batch_size=32):\n",
        "#   n_samples, n_features = X.shape\n",
        "#   n_batches = n_samples\n",
        "#   theta = np.random.randn(n_features)\n",
        "#   for epoch in range(n_epochs):\n",
        "#     indices = np.arange(n_samples)\n",
        "#     np.random.shuffle(indices)\n",
        "#     X_shuffled = X[indices]\n",
        "#     y_shuffled = y[indices]\n",
        "#     for batch_idx in range(n_batches):\n",
        "#       start_idx = batch_idx * batch_size\n",
        "#       end_idx = (batch_idx + 1) * batch_size\n",
        "#       X_batch = X_shuffled[start_idx:end_idx]\n",
        "#       y_batch = y_shuffled[start_idx:end_idx]\n",
        "\n",
        "#       gradient = compute_gradient(X_batch, y_batch, theta)\n",
        "#       theta -= learning_rate * gradient\n",
        "\n",
        "#     mse = np.mean((X.dot(theta) - y) ** 2)\n",
        "#     print(f\"Epoch {epoch+1}/{n_epochs}, Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "#   return theta"
      ],
      "metadata": {
        "id": "tGmaXfaKv5Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_gradient(X, y, weights):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the mean squared error loss with respect to weights.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Input features (m samples, n features).\n",
        "        y (numpy.ndarray): Target values (m samples).\n",
        "        weights (numpy.ndarray): Model weights (n features).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Gradient of the loss with respect to weights (n features).\n",
        "    \"\"\"\n",
        "    m = len(X)\n",
        "    y_pred = np.dot(X, weights)\n",
        "    gradient = -(2/m) * np.dot(X.T, (y - y_pred))\n",
        "    return gradient\n",
        "\n",
        "def stochastic_gradient_descent(X, y, learning_rate=0.01, num_epochs=100, batch_size=32):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent to train a linear regression model.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Input features (m samples, n features).\n",
        "        y (numpy.ndarray): Target values (m samples).\n",
        "        learning_rate (float): Learning rate for SGD.\n",
        "        num_epochs (int): Number of epochs (iterations) for training.\n",
        "        batch_size (int): Size of mini-batches.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Final weights of the linear regression model.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    weights = np.zeros(n)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, m, batch_size):\n",
        "            indices = np.random.choice(m, batch_size, replace=False)\n",
        "            xi = X[indices]\n",
        "            yi = y[indices]\n",
        "            gradient = compute_gradient(xi, yi, weights)\n",
        "            weights -= learning_rate * gradient\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "xYKPVwiVxSUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target"
      ],
      "metadata": {
        "id": "ZGqP7ltswl3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XQfdbiIiwub7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "M7dKiM49wxE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a bias term (intercept) to the features\n",
        "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "X_test = np.c_[np.ones(X_test.shape[0]), X_test]"
      ],
      "metadata": {
        "id": "3Qorv-7ew2WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the linear regression model using SGD\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "final_weights = stochastic_gradient_descent(X_train, y_train, learning_rate, num_epochs)"
      ],
      "metadata": {
        "id": "0sLssC7iw5Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = np.dot(X_test, final_weights)"
      ],
      "metadata": {
        "id": "ZagWAz8Zw7lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean squared error for evaluation\n",
        "mse = np.mean((y_pred - y_test)**2)\n",
        "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wUj_oZtxCYS",
        "outputId": "e7e9d283-27cd-4c22-dc7d-45906d78e8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Set: 2919.4586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0t691qSxFIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}